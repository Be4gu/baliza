# Rust Kickoff Tracker - Balizas ğŸ¯

Una aplicaciÃ³n completa para hacer scraping en tiempo real de eventos de Rust Kickoff, especialmente diseÃ±ada para trackear el estado de las balizas y calcular cuÃ¡ndo estarÃ¡n disponibles nuevamente.

## ğŸš€ CaracterÃ­sticas

- **Scraping automÃ¡tico cada minuto** de https://rustkickoff.com/leaderboards
- **Dashboard en tiempo real** con estado de balizas y countdown timers
- **Filtros avanzados** por tipo de evento, equipo y fecha
- **API REST completa** para acceder a todos los datos
- **Base de datos PostgreSQL** para almacenamiento persistente
- **Interfaz responsive** que funciona en desktop y mÃ³vil

## ğŸ—ï¸ Arquitectura

- **Backend**: Node.js + Express
- **Scraping**: Cheerio + Axios
- **Base de datos**: PostgreSQL (Neon) + Prisma ORM
- **Cron jobs**: node-cron para automatizaciÃ³n
- **Frontend**: HTML5 + CSS3 + JavaScript Vanilla

## ğŸ“ Estructura del proyecto

```
rust-kickoff-tracker/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraper.js          # LÃ³gica de scraping
â”‚   â”œâ”€â”€ database.js         # Servicios de base de datos
â”‚   â””â”€â”€ cronService.js      # Trabajos programados
â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ index.html          # Frontend principal
â”‚   â”œâ”€â”€ styles.css          # Estilos
â”‚   â””â”€â”€ script.js           # LÃ³gica del frontend
â”œâ”€â”€ prisma/
â”‚   â””â”€â”€ schema.prisma       # Esquema de base de datos
â”œâ”€â”€ server.js               # Servidor principal
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

## ğŸ› ï¸ InstalaciÃ³n y configuraciÃ³n

### 1. Requisitos previos

- Node.js 18+
- Una base de datos PostgreSQL (recomendado: Neon.tech)

### 2. Clonar y configurar

```bash
# Instalar dependencias
npm install

# Configurar variables de entorno
cp .env.example .env
```

### 3. Configurar base de datos

Edita `.env` con tu URL de PostgreSQL de Neon:

```env
DATABASE_URL="postgresql://username:password@host:5432/database"
PORT=3000
```

### 4. Configurar Prisma

```bash
# Generar cliente de Prisma
npx prisma generate

# Aplicar migraciones
npx prisma db push
```

### 5. Iniciar la aplicaciÃ³n

```bash
# Desarrollo
npm run dev

# ProducciÃ³n
npm start
```

## ğŸŒ Endpoints de la API

### Eventos

- `GET /api/events` - Todos los eventos con filtros opcionales
- `GET /api/events/balizas` - Solo eventos de balizas

### Balizas

- `GET /api/balizas/status` - Estado actual de todas las balizas

### EstadÃ­sticas

- `GET /api/stats` - EstadÃ­sticas generales del servidor

### Control de scraping

- `GET /api/scraping/status` - Estado del sistema de scraping
- `POST /api/scraping/run` - Ejecutar scraping manualmente
- `POST /api/scraping/restart` - Reiniciar servicios

## ğŸ® Funcionalidades de balizas

### LÃ³gica de balizas

1. Cuando un equipo captura una baliza, queda **ocupada por 1 hora**
2. El sistema calcula automÃ¡ticamente cuÃ¡ndo estarÃ¡ disponible
3. Se muestra un **countdown en tiempo real** hasta que estÃ© disponible
4. Las balizas se marcan como disponibles automÃ¡ticamente

### Dashboard de balizas

- **Estado visual** de cada baliza (disponible/ocupada)
- **Countdown timer** para balizas ocupadas
- **InformaciÃ³n del equipo** que la controla
- **Colores por equipo** para identificaciÃ³n rÃ¡pida

## ğŸ“Š Panel de control

### Funciones principales

- **Auto-refresh** cada 30 segundos del dashboard
- **Scraping manual** para obtener datos inmediatamente
- **Filtros en tiempo real** por evento, equipo y fecha
- **PaginaciÃ³n** para navegar por grandes volÃºmenes de datos

## ğŸš€ Opciones de deployment

### OpciÃ³n 1: AplicaciÃ³n completa en Railway/Render (RECOMENDADO)

**Railway (Gratis con limitaciones)**

1. Fork el repositorio en GitHub
2. Conecta tu cuenta de Railway a GitHub
3. Importa el proyecto y configura la variable `DATABASE_URL`
4. Railway detectarÃ¡ automÃ¡ticamente que es una app Node.js
5. La app estarÃ¡ disponible 24/7 con scraping automÃ¡tico

**Render (Gratis con limitaciones)**

1. Conecta tu repositorio a Render
2. Configura como Web Service
3. AÃ±ade la variable de entorno `DATABASE_URL`
4. Deploy automÃ¡tico

### OpciÃ³n 2: Local + Frontend estÃ¡tico

**Si prefieres mantener el backend local:**

1. **Backend local**: Ejecuta `npm start` en tu mÃ¡quina
2. **Frontend en Vercel/Netlify**:
   - Modifica las URLs de la API en `script.js` para apuntar a tu IP local
   - Deploy solo la carpeta `public/` en Vercel o Netlify

### OpciÃ³n 3: Docker (para deployment propio)

```dockerfile
FROM node:18-alpine
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production
COPY . .
RUN npx prisma generate
EXPOSE 3000
CMD ["npm", "start"]
```

## âš¡ Consideraciones de performance

### Limitaciones de planes gratuitos

- **Railway**: 500 horas/mes (suficiente para uso continuo)
- **Render**: Se "duerme" despuÃ©s de inactividad, puede tardar en despertar
- **Vercel**: No es ideal para cron jobs largos

### RecomendaciÃ³n

Para uso serio, usa **Railway** o mantÃ©n el backend **local** con frontend estÃ¡tico.

## ğŸ”§ ConfiguraciÃ³n avanzada

### Ajustar frecuencia de scraping

En `src/cronService.js`, lÃ­nea 26:

```javascript
// Cada minuto
this.scrapingJob = cron.schedule('*/1 * * * *', ...);

// Cada 30 segundos (para testing)
this.scrapingJob = cron.schedule('*/30 * * * * *', ...);

// Cada 5 minutos (para reducir carga)
this.scrapingJob = cron.schedule('*/5 * * * *', ...);
```

### Personalizar detecciÃ³n de balizas

En `src/database.js`, mÃ©todo `extractBalizaId()` puedes ajustar los patrones para detectar diferentes tipos de balizas segÃºn aparezcan en los eventos.

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para detalles.

## ğŸ¯ Tu aplicaciÃ³n estÃ¡ LISTA para producciÃ³n

### âœ… ConfiguraciÃ³n completada:

- ğŸ—„ï¸ **Base de datos PostgreSQL** configurada con Neon
- ğŸ”„ **VerificaciÃ³n de eventos duplicados** implementada y probada
- â±ï¸ **Sistema de timestamps** que compara Ãºltimo evento vs scrapeados
- ğŸŒ **Scripts de configuraciÃ³n** para cambio dev/prod automÃ¡tico
- ğŸ“¦ **Build scripts** optimizados para deployment

### ğŸš€ PrÃ³ximos pasos para desplegar:

1. **Ejecutar configuraciÃ³n de producciÃ³n**:

   ```bash
   npm run setup:prod
   ```

2. **Subir a tu plataforma favorita** (Railway, Render, Vercel)

3. **Configurar variable de entorno**:

   ```
   DATABASE_URL=postgresql://neondb_owner:npg_2vzRpDT5MUIP@ep-flat-block-abfilz7u-pooler.eu-west-2.aws.neon.tech/neondb?sslmode=require&channel_binding=require
   ```

4. **Â¡Listo!** La aplicaciÃ³n comenzarÃ¡ a hacer scraping automÃ¡ticamente y acumularÃ¡ datos en la nube

### ğŸ”¥ Funcionalidades listas:

- âš¡ Scraping cada minuto con verificaciÃ³n de duplicados
- ğŸ—„ï¸ Los datos se acumularÃ¡n progresivamente en PostgreSQL
- ğŸ¯ Solo procesa eventos nuevos (no duplica datos)
- ğŸŒ Frontend simplificado enfocado solo en balizas
- â±ï¸ Countdown timers de 60 minutos para balizas ocupadas

## âš ï¸ Disclaimer

Este proyecto es para fines educativos y de entretenimiento. AsegÃºrate de cumplir con los tÃ©rminos de servicio de rustkickoff.com al hacer scraping de su contenido.

---

Desarrollado con â¤ï¸ para la comunidad de Rust
